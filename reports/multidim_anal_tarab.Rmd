---
title: "multidimens_tarab"
output: html_document
---
```{r, include =FALSE}
library (randomForest)
library (ica)
library (e1071) #svm

library (tidyverse)
library (stringr)
#for plotting
library(gridExtra)
library(RGraphics)
source ("../Rcode/functions.r")
```


I am using the lehard data to test the multidimensional data.



```{r, evaluate =FALSE}
load("multidim_Tarabykin.rdata")
summary (as.factor(metadata$groupingvar))

numberofvariables = (length(names(behav_gp))-3)* nrow (Timewindows)
numberofvariables = trunc(numberofvariables/3)

```
The data is grouped by `r Projects_metadata$group_by `.
Data transformation: `r calcul_text`.


---


We grouped the variables following the `r groupingby` argument to get `r length(names(behav_gp))-3` behavior categories. We used the folowing time windows and got `r length(names(behav_gp))-3` x `r nrow (Timewindows)` = `r (length(names(behav_gp))-3)* nrow (Timewindows)` variables :

```{r, results='asis'}
pander::pandoc.table(Timewindows)
```
Note that the last window might be truncated if not all dataset is achieving 900 min after light on.

We then run a random forest to get the variables in order of importance to distinguish the groups.

We tried to tune the mtry parameter, but the function "tuneRF" gives different minima each time we run it, so we decided to keep the default value (sqrt(p), here p = `r (length(names(behav_gp))-3)* nrow (Timewindows)` ).

```{r, eval=FALSE}
tuneRF(Multi_datainput_m%>% select (-groupingvar),Multi_datainput_m$groupingvar, ntreeTry=50, stepFactor=2, improve=0.05,
       trace=TRUE, plot=TRUE, doBest=FALSE)

tuneRF(Multi_datainput_m%>% select (-groupingvar),Multi_datainput_m$groupingvar, ntreeTry=50, stepFactor=2, improve=0.05,
       trace=TRUE, plot=TRUE, doBest=FALSE)
```



We therefore used the random forest using the default mtry value and plot here the table of variables ordered by weight:
```{r}
HCS.rf <- randomForest(groupingvar ~ ., data=Multi_datainput_m, importance=TRUE,
                        proximity=TRUE, ntree =1500)

R =round(importance(HCS.rf, type=2), 2)
R2=data.frame(row.names (R),R)  %>% arrange(-MeanDecreaseGini)

#pander::pandoc.table(R2)
varImpPlot(HCS.rf)
```


let's try to get only the `r numberofvariables` first variables and run the random forest again:

```{r}


Input =Multi_datainput_m [,names(Multi_datainput_m) %in% as.character(R2 [1:numberofvariables,1]) ]
Input$groupingvar =Multi_datainput_m$groupingvar

HCS.rf2 <- randomForest(groupingvar ~ ., data=Input, importance=TRUE,
                        proximity=TRUE, ntree =1500)
R =round(importance(HCS.rf2, type=2), 2)
R2=data.frame(row.names (R),R)  %>% arrange(-MeanDecreaseGini)

#pander::pandoc.table(R2)

varImpPlot(HCS.rf2)
```

Let's take a teshold of of importance import_treshold and get all variables giving a MeanDecreaseGini over it (NB: THIS VARIABLE MIGHT NEED TO BE SET FOR EACH DATA), at least 6 data points:

```{r}
 import_treshold =0.95

R3=data.frame(row.names (R),R)  %>% 
  filter(MeanDecreaseGini > import_treshold)
numberofvariables =max (nrow (R3), 6)

Input =Multi_datainput_m [,names(Multi_datainput_m) %in% as.character(R2 [1:numberofvariables,1]) ]
Input$groupingvar =Multi_datainput_m$groupingvar

HCS.rf2 <- randomForest(groupingvar ~ ., data=Input, importance=TRUE,
                        proximity=TRUE, ntree =1500)
varImpPlot(HCS.rf2)
```

#Plotting
First, lets plot the 2 most discriminative variables following the random forest:

```{r}


 Plot = Multi_datainput_m [,names(Multi_datainput_m) %in% as.character(R2 [1:2,1]) ]
  Plot = cbind(Multi_datainput_m$groupingvar, Plot)
  Title_plot = paste0(names (Plot) [2],"x",names (Plot) [3])
  names (Plot) = c("groupingvar","disciminant1", "discriminant2")
  p=ggplot (Plot, aes (y= disciminant1, x=discriminant2, color= groupingvar))+
    geom_point()+
    labs(title = Title_plot)+
    #scale_x_log10() + scale_y_log10()+
    scale_colour_grey() + theme_bw()+
      theme(legend.position='none')
print(p)  
```


Now is the time to plot the main components. For this, we will make an ICA on the reduced data and plot the first two components:

```{r}
 p=icafast(Input%>% select (-groupingvar),2,center=T,maxit=100)

R= cbind(p$Y, Input   %>% select (groupingvar))
    names(R) = c("D1", "D2",  "groupingvar")
    pls=R %>% ggplot (aes (x=D1, y=D2, color = groupingvar))+
      geom_point()+
      labs (title=numberofvariables)+ 
      scale_colour_grey() + theme_bw()+
      theme(legend.position='none')
print(pls)    
```





#SVM
 
We perform a SVM on the total data or the reduced data and compare the results. For that with split the data in training and test sets, tune the svm for best parameters and then run the svm and gives the overall accuracy as the output.
(the optimization of the parameters is computer intensive and may take time).

!!!WHAT KERNEL TO USE; USE ALL DATA OR ONLY A SUBSET; P VALUE 

```{r}
##data input
Input =Multi_datainput_m [,names(Multi_datainput_m) %in% c(as.character(R2 [1:numberofvariables,1]), "groupingvar") ]

##data splitting  
  L=levels(Input$groupingvar)
  Glass= Input %>% filter (groupingvar == L[1])
  Glass2= Input %>% filter (groupingvar == L[2])
  
  if (nrow (Glass) != nrow (Glass2)) stop("the groups do not have the same size !")
  
  index     <- 1:nrow(Glass )
  testindex <- sample(index, trunc(length(index)/3))
  testset   <- rbind(Glass[testindex,],Glass2[testindex,])
  trainset  <- rbind(Glass[-testindex,],Glass2[-testindex,])
##tuning and performing svm  
  bestk= tune.svm2(trainset,groupingvar)
  obj= NA
  obj$best.parameters = bestk[[2]]
 
  
  svm.model <- svm(groupingvar ~ ., data = trainset, cost = obj$best.parameters$cost, gamma = obj$best.parameters$gamma, kernel = bestk[[1]])
  svm.pred <- predict(svm.model, trainset %>% select(-groupingvar))

SVMprediction_res =table(pred = svm.pred, true = trainset$groupingvar)
SVMprediction = as.data.frame(SVMprediction_res)

#Accuracy of grouping and plot
temp =classAgreement (SVMprediction_res)
subsetcrand=temp$crand
subsetmodel=svm.model

  svm.pred <- predict(svm.model, testset %>% select(-groupingvar))
  
  SVMprediction_res =table(pred = svm.pred, true = testset$groupingvar)
  SVMprediction = as.data.frame(SVMprediction_res)
  
  #Accuracy of grouping and plot
  temp =classAgreement (SVMprediction_res)
  Accuracy =paste0(ncol(Input)-1," variables: Accuracy of the prediction with ",bestk[[1]]," kernel (Corrected Rand index: 0 denotes chance level, maximum is 1):",temp$crand)
  
  print(Accuracy)
  
  
```

```{r}
#set.seed(71)
##data input)
Input =Multi_datainput_m #%>% select (-ID)
##data splitting  
  L=levels(Input$groupingvar)
  Glass= Input %>% filter (groupingvar == L[1])
  Glass2= Input %>% filter (groupingvar == L[2])
  
  if (nrow (Glass) != nrow (Glass2)) stop("the groups do not have the same size !")
  
  #index     <- 1:nrow(Glass )
  #testindex <- sample(index, trunc(length(index)/3))
  #we take the same data as for the previous run
  testset   <- rbind(Glass[testindex,],Glass2[testindex,])
  trainset  <- rbind(Glass[-testindex,],Glass2[-testindex,])
##tuning and performing svm  
  bestk= tune.svm2(trainset,groupingvar)
  obj$best.parameters = bestk[[2]]
 
  
  svm.model <- svm(groupingvar ~ ., data = trainset, cost = obj$best.parameters$cost, gamma = obj$best.parameters$gamma, kernel = bestk[[1]])
  svm.pred <- predict(svm.model, testset %>% select(-groupingvar))
  
  SVMprediction_res =table(pred = svm.pred, true = testset$groupingvar)
  SVMprediction = as.data.frame(SVMprediction_res)
  
  #Accuracy of grouping and plot
  temp =classAgreement (SVMprediction_res)
  Accuracy =paste0(ncol(Input)-1," variables: Accuracy of the prediction with ",bestk[[1]]," kernel (Corrected Rand index: 0 denotes chance level, maximum is 1):",temp$crand)
  
  print(Accuracy)
```
We will now use permutation to see if the computer can tell the two groups apart. What it does is permute the elements in random groups, calculate an accuracy score, then we will see if the accuracy we have is meaning that the two groups can actually be separated.










